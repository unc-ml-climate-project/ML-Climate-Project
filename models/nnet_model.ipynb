{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 4)\n",
      "(22, 1)\n",
      "(22, 4)\n",
      "(22, 1)\n"
     ]
    }
   ],
   "source": [
    "# Select target country\n",
    "target_country = 'Indonesia'\n",
    "\n",
    "# Read the CSV File for the target country\n",
    "file_path = os.path.join(os.getcwd(), f\"clean_data/master_dataset_{target_country}.csv\")  # Define the file path\n",
    "train_data = pd.read_csv(file_path)  # Read the CSV file into a DataFrame\n",
    "input_train_data = train_data[['Year', 'total_co2_emissions', 'sea_level_rise', 'average_extreme_events']]\n",
    "output_train_data = train_data[['avg_global_temp']]\n",
    "print(input_train_data.shape)\n",
    "print(output_train_data.shape)\n",
    "\n",
    "# Select target country\n",
    "test_country = 'Argentina'\n",
    "\n",
    "# Read the CSV File for the target country\n",
    "file_path = os.path.join(os.getcwd(), f\"clean_data/master_dataset_{test_country}.csv\")  # Define the file path\n",
    "test_data = pd.read_csv(file_path)  # Read the CSV file into a DataFrame\n",
    "input_test_data = test_data[['Year', 'total_co2_emissions', 'sea_level_rise', 'average_extreme_events']]\n",
    "output_test_data = test_data[['avg_global_temp']]\n",
    "print(input_train_data.shape)\n",
    "print(output_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "num_years_train = train_data['Year'].nunique() # Batch size\n",
    "num_years_test = test_data['Year'].nunique() # Batch size\n",
    "print(f\"{num_years_train}\\n{num_years_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_loss(X, Y, nnet):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: Num_years x 4 with dimensions of co2, sea level rise, and extreme weather (for given year)\n",
    "    Y: N_batch x \n",
    "  Returns:\n",
    "    loss: scalar of mse_regression_loss batch\n",
    "  \"\"\"\n",
    "  nnet_output = nnet(X)\n",
    "\n",
    "  # mse_loss should be a mean of the squared error of the\n",
    "  mse_loss = tf.reduce_mean(tf.square(nnet_output - Y)) # Linear squared error calculation\n",
    "  return mse_loss\n",
    "\n",
    "def grad(X, Y, nnet):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = temp_loss(X, Y, nnet)\n",
    "\n",
    "  return tape.gradient(loss_value, nnet.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">726</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m)               │           \u001b[38;5;34m726\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,942</span> (7.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,942\u001b[0m (7.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,942</span> (7.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,942\u001b[0m (7.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnet = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_years_train, activation='relu') # Linear output layer\n",
    "]) # Model with 2 hidden layers with 256 hidden units each (relu activation)\n",
    "nnet.build([num_years_train, 4]) # Use downsampled image as input\n",
    "nnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 32332.520\n",
      "Loss at step 001: 31994.330\n",
      "Loss at step 002: 31662.086\n",
      "Loss at step 003: 31336.328\n",
      "Loss at step 004: 31014.762\n",
      "Loss at step 005: 30696.500\n",
      "Loss at step 006: 30381.012\n",
      "Loss at step 007: 30068.330\n",
      "Loss at step 008: 29758.715\n",
      "Loss at step 009: 29452.607\n",
      "Loss at step 010: 29151.023\n",
      "Loss at step 011: 28856.301\n",
      "Loss at step 012: 28569.002\n",
      "Loss at step 013: 28285.967\n",
      "Loss at step 014: 28006.527\n",
      "Loss at step 015: 27729.857\n",
      "Loss at step 016: 27455.992\n",
      "Loss at step 017: 27185.082\n",
      "Loss at step 018: 26917.141\n",
      "Loss at step 019: 26652.887\n",
      "Loss at step 020: 26393.062\n",
      "Loss at step 021: 26136.943\n",
      "Loss at step 022: 25883.736\n",
      "Loss at step 023: 25633.361\n",
      "Loss at step 024: 25385.539\n",
      "Loss at step 025: 25140.219\n",
      "Loss at step 026: 24897.381\n",
      "Loss at step 027: 24657.025\n",
      "Loss at step 028: 24419.131\n",
      "Loss at step 029: 24183.686\n",
      "Loss at step 030: 23950.678\n",
      "Loss at step 031: 23720.078\n",
      "Loss at step 032: 23491.877\n",
      "Loss at step 033: 23266.045\n",
      "Loss at step 034: 23042.570\n",
      "Loss at step 035: 22821.430\n",
      "Loss at step 036: 22602.602\n",
      "Loss at step 037: 22386.062\n",
      "Loss at step 038: 22171.785\n",
      "Loss at step 039: 21959.750\n",
      "Loss at step 040: 21749.938\n",
      "Loss at step 041: 21542.396\n",
      "Loss at step 042: 21338.645\n",
      "Loss at step 043: 21137.826\n",
      "Loss at step 044: 20939.059\n",
      "Loss at step 045: 20742.293\n",
      "Loss at step 046: 20547.523\n",
      "Loss at step 047: 20354.711\n",
      "Loss at step 048: 20163.836\n",
      "Loss at step 049: 19974.873\n",
      "Loss at step 050: 19787.799\n",
      "Loss at step 051: 19602.596\n",
      "Loss at step 052: 19419.229\n",
      "Loss at step 053: 19237.684\n",
      "Loss at step 054: 19057.938\n",
      "Loss at step 055: 18879.967\n",
      "Loss at step 056: 18703.752\n",
      "Loss at step 057: 18529.281\n",
      "Loss at step 058: 18356.547\n",
      "Loss at step 059: 18185.508\n",
      "Loss at step 060: 18016.129\n",
      "Loss at step 061: 17848.398\n",
      "Loss at step 062: 17682.297\n",
      "Loss at step 063: 17517.807\n",
      "Loss at step 064: 17354.906\n",
      "Loss at step 065: 17193.578\n",
      "Loss at step 066: 17033.812\n",
      "Loss at step 067: 16875.611\n",
      "Loss at step 068: 16718.934\n",
      "Loss at step 069: 16563.771\n",
      "Loss at step 070: 16410.105\n",
      "Loss at step 071: 16257.924\n",
      "Loss at step 072: 16107.221\n",
      "Loss at step 073: 15957.963\n",
      "Loss at step 074: 15810.126\n",
      "Loss at step 075: 15663.700\n",
      "Loss at step 076: 15518.667\n",
      "Loss at step 077: 15375.011\n",
      "Loss at step 078: 15232.720\n",
      "Loss at step 079: 15091.771\n",
      "Loss at step 080: 14952.159\n",
      "Loss at step 081: 14813.871\n",
      "Loss at step 082: 14676.883\n",
      "Loss at step 083: 14541.184\n",
      "Loss at step 084: 14406.758\n",
      "Loss at step 085: 14273.602\n",
      "Loss at step 086: 14141.701\n",
      "Loss at step 087: 14011.032\n",
      "Loss at step 088: 13881.585\n",
      "Loss at step 089: 13753.354\n",
      "Loss at step 090: 13626.324\n",
      "Loss at step 091: 13500.482\n",
      "Loss at step 092: 13375.810\n",
      "Loss at step 093: 13252.301\n",
      "Loss at step 094: 13129.944\n",
      "Loss at step 095: 13008.725\n",
      "Loss at step 096: 12888.630\n",
      "Loss at step 097: 12769.650\n",
      "Loss at step 098: 12651.774\n",
      "Loss at step 099: 12534.995\n",
      "Loss at step 100: 12419.294\n",
      "Loss at step 101: 12304.664\n",
      "Loss at step 102: 12191.094\n",
      "Loss at step 103: 12078.571\n",
      "Loss at step 104: 11967.089\n",
      "Loss at step 105: 11856.635\n",
      "Loss at step 106: 11747.194\n",
      "Loss at step 107: 11638.770\n",
      "Loss at step 108: 11531.344\n",
      "Loss at step 109: 11424.900\n",
      "Loss at step 110: 11319.445\n",
      "Loss at step 111: 11214.950\n",
      "Loss at step 112: 11111.411\n",
      "Loss at step 113: 11008.826\n",
      "Loss at step 114: 10907.181\n",
      "Loss at step 115: 10806.467\n",
      "Loss at step 116: 10706.675\n",
      "Loss at step 117: 10607.793\n",
      "Loss at step 118: 10509.818\n",
      "Loss at step 119: 10412.738\n",
      "Loss at step 120: 10316.546\n",
      "Loss at step 121: 10221.230\n",
      "Loss at step 122: 10126.787\n",
      "Loss at step 123: 10033.213\n",
      "Loss at step 124: 9940.481\n",
      "Loss at step 125: 9848.598\n",
      "Loss at step 126: 9757.552\n",
      "Loss at step 127: 9667.335\n",
      "Loss at step 128: 9577.940\n",
      "Loss at step 129: 9489.357\n",
      "Loss at step 130: 9401.578\n",
      "Loss at step 131: 9314.603\n",
      "Loss at step 132: 9228.415\n",
      "Loss at step 133: 9143.013\n",
      "Loss at step 134: 9058.386\n",
      "Loss at step 135: 8974.524\n",
      "Loss at step 136: 8891.426\n",
      "Loss at step 137: 8809.082\n",
      "Loss at step 138: 8727.483\n",
      "Loss at step 139: 8646.627\n",
      "Loss at step 140: 8566.511\n",
      "Loss at step 141: 8487.113\n",
      "Loss at step 142: 8408.433\n",
      "Loss at step 143: 8330.471\n",
      "Loss at step 144: 8253.215\n",
      "Loss at step 145: 8176.297\n",
      "Loss at step 146: 8098.961\n",
      "Loss at step 147: 8022.015\n",
      "Loss at step 148: 7945.659\n",
      "Loss at step 149: 7869.965\n",
      "Loss at step 150: 7794.984\n",
      "Loss at step 151: 7720.747\n",
      "Loss at step 152: 7647.281\n",
      "Loss at step 153: 7574.607\n",
      "Loss at step 154: 7502.729\n",
      "Loss at step 155: 7431.662\n",
      "Loss at step 156: 7361.387\n",
      "Loss at step 157: 7291.908\n",
      "Loss at step 158: 7223.218\n",
      "Loss at step 159: 7155.302\n",
      "Loss at step 160: 7088.144\n",
      "Loss at step 161: 7021.723\n",
      "Loss at step 162: 6956.023\n",
      "Loss at step 163: 6891.021\n",
      "Loss at step 164: 6826.696\n",
      "Loss at step 165: 6763.025\n",
      "Loss at step 166: 6699.989\n",
      "Loss at step 167: 6637.568\n",
      "Loss at step 168: 6575.744\n",
      "Loss at step 169: 6514.503\n",
      "Loss at step 170: 6453.828\n",
      "Loss at step 171: 6393.707\n",
      "Loss at step 172: 6334.129\n",
      "Loss at step 173: 6275.087\n",
      "Loss at step 174: 6216.578\n",
      "Loss at step 175: 6158.584\n",
      "Loss at step 176: 6101.110\n",
      "Loss at step 177: 6044.146\n",
      "Loss at step 178: 5987.689\n",
      "Loss at step 179: 5931.737\n",
      "Loss at step 180: 5876.284\n",
      "Loss at step 181: 5821.328\n",
      "Loss at step 182: 5766.864\n",
      "Loss at step 183: 5712.889\n",
      "Loss at step 184: 5659.401\n",
      "Loss at step 185: 5606.393\n",
      "Loss at step 186: 5553.865\n",
      "Loss at step 187: 5501.812\n",
      "Loss at step 188: 5450.229\n",
      "Loss at step 189: 5399.113\n",
      "Loss at step 190: 5348.462\n",
      "Loss at step 191: 5298.270\n",
      "Loss at step 192: 5248.533\n",
      "Loss at step 193: 5199.246\n",
      "Loss at step 194: 5150.407\n",
      "Loss at step 195: 5102.725\n",
      "Loss at step 196: 5057.970\n",
      "Loss at step 197: 5015.639\n",
      "Loss at step 198: 4974.846\n",
      "Loss at step 199: 4934.516\n",
      "Loss at step 200: 4894.389\n",
      "Loss at step 201: 4854.482\n",
      "Loss at step 202: 4814.806\n",
      "Loss at step 203: 4775.371\n",
      "Loss at step 204: 4736.182\n",
      "Loss at step 205: 4697.248\n",
      "Loss at step 206: 4658.572\n",
      "Loss at step 207: 4620.162\n",
      "Loss at step 208: 4582.020\n",
      "Loss at step 209: 4544.148\n",
      "Loss at step 210: 4506.551\n",
      "Loss at step 211: 4469.227\n",
      "Loss at step 212: 4432.181\n",
      "Loss at step 213: 4395.413\n",
      "Loss at step 214: 4358.924\n",
      "Loss at step 215: 4322.713\n",
      "Loss at step 216: 4286.782\n",
      "Loss at step 217: 4251.130\n",
      "Loss at step 218: 4215.758\n",
      "Loss at step 219: 4180.662\n",
      "Loss at step 220: 4145.846\n",
      "Loss at step 221: 4111.305\n",
      "Loss at step 222: 4077.039\n",
      "Loss at step 223: 4043.049\n",
      "Loss at step 224: 4009.331\n",
      "Loss at step 225: 3975.885\n",
      "Loss at step 226: 3942.687\n",
      "Loss at step 227: 3909.737\n",
      "Loss at step 228: 3877.051\n",
      "Loss at step 229: 3844.573\n",
      "Loss at step 230: 3812.312\n",
      "Loss at step 231: 3780.267\n",
      "Loss at step 232: 3748.407\n",
      "Loss at step 233: 3716.733\n",
      "Loss at step 234: 3685.193\n",
      "Loss at step 235: 3653.814\n",
      "Loss at step 236: 3622.604\n",
      "Loss at step 237: 3591.599\n",
      "Loss at step 238: 3560.802\n",
      "Loss at step 239: 3530.242\n",
      "Loss at step 240: 3499.940\n",
      "Loss at step 241: 3469.898\n",
      "Loss at step 242: 3440.077\n",
      "Loss at step 243: 3410.557\n",
      "Loss at step 244: 3381.312\n",
      "Loss at step 245: 3352.323\n",
      "Loss at step 246: 3323.574\n",
      "Loss at step 247: 3295.009\n",
      "Loss at step 248: 3266.631\n",
      "Loss at step 249: 3238.442\n",
      "Loss at step 250: 3210.447\n",
      "Loss at step 251: 3182.646\n",
      "Loss at step 252: 3155.041\n",
      "Loss at step 253: 3127.634\n",
      "Loss at step 254: 3100.438\n",
      "Loss at step 255: 3073.513\n",
      "Loss at step 256: 3046.801\n",
      "Loss at step 257: 3020.308\n",
      "Loss at step 258: 2994.043\n",
      "Loss at step 259: 2968.013\n",
      "Loss at step 260: 2942.192\n",
      "Loss at step 261: 2916.573\n",
      "Loss at step 262: 2891.154\n",
      "Loss at step 263: 2865.936\n",
      "Loss at step 264: 2840.916\n",
      "Loss at step 265: 2816.096\n",
      "Loss at step 266: 2791.473\n",
      "Loss at step 267: 2767.048\n",
      "Loss at step 268: 2742.818\n",
      "Loss at step 269: 2718.784\n",
      "Loss at step 270: 2694.942\n",
      "Loss at step 271: 2671.294\n",
      "Loss at step 272: 2647.843\n",
      "Loss at step 273: 2624.594\n",
      "Loss at step 274: 2601.539\n",
      "Loss at step 275: 2578.672\n",
      "Loss at step 276: 2555.993\n",
      "Loss at step 277: 2533.500\n",
      "Loss at step 278: 2511.190\n",
      "Loss at step 279: 2489.059\n",
      "Loss at step 280: 2467.110\n",
      "Loss at step 281: 2445.343\n",
      "Loss at step 282: 2423.756\n",
      "Loss at step 283: 2402.348\n",
      "Loss at step 284: 2381.117\n",
      "Loss at step 285: 2360.063\n",
      "Loss at step 286: 2339.188\n",
      "Loss at step 287: 2318.488\n",
      "Loss at step 288: 2297.961\n",
      "Loss at step 289: 2277.603\n",
      "Loss at step 290: 2257.415\n",
      "Loss at step 291: 2237.394\n",
      "Loss at step 292: 2217.543\n",
      "Loss at step 293: 2197.858\n",
      "Loss at step 294: 2178.344\n",
      "Loss at step 295: 2158.992\n",
      "Loss at step 296: 2139.802\n",
      "Loss at step 297: 2120.775\n",
      "Loss at step 298: 2101.907\n",
      "Loss at step 299: 2083.199\n",
      "Loss at step 300: 2064.649\n",
      "Loss at step 301: 2046.259\n",
      "Loss at step 302: 2028.025\n",
      "Loss at step 303: 2009.946\n",
      "Loss at step 304: 1992.021\n",
      "Loss at step 305: 1974.248\n",
      "Loss at step 306: 1956.629\n",
      "Loss at step 307: 1939.160\n",
      "Loss at step 308: 1921.841\n",
      "Loss at step 309: 1904.671\n",
      "Loss at step 310: 1887.648\n",
      "Loss at step 311: 1870.772\n",
      "Loss at step 312: 1854.040\n",
      "Loss at step 313: 1837.453\n",
      "Loss at step 314: 1821.010\n",
      "Loss at step 315: 1804.708\n",
      "Loss at step 316: 1788.548\n",
      "Loss at step 317: 1772.528\n",
      "Loss at step 318: 1756.647\n",
      "Loss at step 319: 1740.904\n",
      "Loss at step 320: 1725.298\n",
      "Loss at step 321: 1709.828\n",
      "Loss at step 322: 1694.493\n",
      "Loss at step 323: 1679.292\n",
      "Loss at step 324: 1664.224\n",
      "Loss at step 325: 1649.288\n",
      "Loss at step 326: 1634.482\n",
      "Loss at step 327: 1619.807\n",
      "Loss at step 328: 1605.261\n",
      "Loss at step 329: 1590.842\n",
      "Loss at step 330: 1576.551\n",
      "Loss at step 331: 1562.386\n",
      "Loss at step 332: 1548.345\n",
      "Loss at step 333: 1534.430\n",
      "Loss at step 334: 1520.637\n",
      "Loss at step 335: 1506.967\n",
      "Loss at step 336: 1493.418\n",
      "Loss at step 337: 1479.989\n",
      "Loss at step 338: 1466.679\n",
      "Loss at step 339: 1453.489\n",
      "Loss at step 340: 1440.415\n",
      "Loss at step 341: 1427.458\n",
      "Loss at step 342: 1414.617\n",
      "Loss at step 343: 1401.892\n",
      "Loss at step 344: 1389.280\n",
      "Loss at step 345: 1376.781\n",
      "Loss at step 346: 1364.395\n",
      "Loss at step 347: 1352.120\n",
      "Loss at step 348: 1339.955\n",
      "Loss at step 349: 1327.900\n",
      "Loss at step 350: 1315.954\n",
      "Loss at step 351: 1304.115\n",
      "Loss at step 352: 1292.383\n",
      "Loss at step 353: 1280.759\n",
      "Loss at step 354: 1269.239\n",
      "Loss at step 355: 1257.824\n",
      "Loss at step 356: 1246.512\n",
      "Loss at step 357: 1235.303\n",
      "Loss at step 358: 1224.197\n",
      "Loss at step 359: 1213.192\n",
      "Loss at step 360: 1202.287\n",
      "Loss at step 361: 1191.482\n",
      "Loss at step 362: 1180.775\n",
      "Loss at step 363: 1170.167\n",
      "Loss at step 364: 1159.656\n",
      "Loss at step 365: 1149.241\n",
      "Loss at step 366: 1138.922\n",
      "Loss at step 367: 1128.697\n",
      "Loss at step 368: 1118.568\n",
      "Loss at step 369: 1108.531\n",
      "Loss at step 370: 1098.587\n",
      "Loss at step 371: 1088.735\n",
      "Loss at step 372: 1078.974\n",
      "Loss at step 373: 1069.303\n",
      "Loss at step 374: 1059.722\n",
      "Loss at step 375: 1050.230\n",
      "Loss at step 376: 1040.826\n",
      "Loss at step 377: 1031.510\n",
      "Loss at step 378: 1022.280\n",
      "Loss at step 379: 1013.136\n",
      "Loss at step 380: 1004.078\n",
      "Loss at step 381: 995.104\n",
      "Loss at step 382: 986.214\n",
      "Loss at step 383: 977.407\n",
      "Loss at step 384: 968.683\n",
      "Loss at step 385: 960.040\n",
      "Loss at step 386: 951.479\n",
      "Loss at step 387: 942.998\n",
      "Loss at step 388: 934.597\n",
      "Loss at step 389: 926.275\n",
      "Loss at step 390: 918.032\n",
      "Loss at step 391: 909.866\n",
      "Loss at step 392: 901.778\n",
      "Loss at step 393: 893.767\n",
      "Loss at step 394: 885.831\n",
      "Loss at step 395: 877.970\n",
      "Loss at step 396: 870.184\n",
      "Loss at step 397: 862.472\n",
      "Loss at step 398: 854.833\n",
      "Loss at step 399: 847.267\n",
      "Loss at step 400: 839.773\n",
      "Loss at step 401: 832.351\n",
      "Loss at step 402: 824.999\n",
      "Loss at step 403: 817.718\n",
      "Loss at step 404: 810.506\n",
      "Loss at step 405: 803.364\n",
      "Loss at step 406: 796.290\n",
      "Loss at step 407: 789.284\n",
      "Loss at step 408: 782.346\n",
      "Loss at step 409: 775.474\n",
      "Loss at step 410: 768.668\n",
      "Loss at step 411: 761.928\n",
      "Loss at step 412: 755.253\n",
      "Loss at step 413: 748.642\n",
      "Loss at step 414: 742.096\n",
      "Loss at step 415: 735.612\n",
      "Loss at step 416: 729.192\n",
      "Loss at step 417: 722.834\n",
      "Loss at step 418: 716.537\n",
      "Loss at step 419: 710.302\n",
      "Loss at step 420: 704.128\n",
      "Loss at step 421: 698.013\n",
      "Loss at step 422: 691.959\n",
      "Loss at step 423: 685.963\n",
      "Loss at step 424: 680.026\n",
      "Loss at step 425: 674.147\n",
      "Loss at step 426: 668.326\n",
      "Loss at step 427: 662.562\n",
      "Loss at step 428: 656.854\n",
      "Loss at step 429: 651.202\n",
      "Loss at step 430: 645.606\n",
      "Loss at step 431: 640.065\n",
      "Loss at step 432: 634.578\n",
      "Loss at step 433: 629.146\n",
      "Loss at step 434: 623.767\n",
      "Loss at step 435: 618.442\n",
      "Loss at step 436: 613.168\n",
      "Loss at step 437: 607.948\n",
      "Loss at step 438: 602.779\n",
      "Loss at step 439: 597.661\n",
      "Loss at step 440: 592.595\n",
      "Loss at step 441: 587.578\n",
      "Loss at step 442: 582.612\n",
      "Loss at step 443: 577.695\n",
      "Loss at step 444: 572.827\n",
      "Loss at step 445: 568.008\n",
      "Loss at step 446: 563.236\n",
      "Loss at step 447: 558.513\n",
      "Loss at step 448: 553.837\n",
      "Loss at step 449: 549.208\n",
      "Loss at step 450: 544.625\n",
      "Loss at step 451: 540.088\n",
      "Loss at step 452: 535.597\n",
      "Loss at step 453: 531.151\n",
      "Loss at step 454: 526.750\n",
      "Loss at step 455: 522.394\n",
      "Loss at step 456: 518.081\n",
      "Loss at step 457: 513.812\n",
      "Loss at step 458: 509.586\n",
      "Loss at step 459: 505.403\n",
      "Loss at step 460: 501.263\n",
      "Loss at step 461: 497.164\n",
      "Loss at step 462: 493.107\n",
      "Loss at step 463: 489.091\n",
      "Loss at step 464: 485.117\n",
      "Loss at step 465: 481.182\n",
      "Loss at step 466: 477.288\n",
      "Loss at step 467: 473.434\n",
      "Loss at step 468: 469.619\n",
      "Loss at step 469: 465.844\n",
      "Loss at step 470: 462.107\n",
      "Loss at step 471: 458.408\n",
      "Loss at step 472: 454.747\n",
      "Loss at step 473: 451.124\n",
      "Loss at step 474: 447.538\n",
      "Loss at step 475: 443.989\n",
      "Loss at step 476: 440.477\n",
      "Loss at step 477: 437.001\n",
      "Loss at step 478: 433.561\n",
      "Loss at step 479: 430.156\n",
      "Loss at step 480: 426.787\n",
      "Loss at step 481: 423.453\n",
      "Loss at step 482: 420.153\n",
      "Loss at step 483: 416.888\n",
      "Loss at step 484: 413.659\n",
      "Loss at step 485: 410.467\n",
      "Loss at step 486: 407.308\n",
      "Loss at step 487: 404.183\n",
      "Loss at step 488: 401.090\n",
      "Loss at step 489: 398.030\n",
      "Loss at step 490: 395.002\n",
      "Loss at step 491: 392.007\n",
      "Loss at step 492: 389.046\n",
      "Loss at step 493: 386.116\n",
      "Loss at step 494: 383.217\n",
      "Loss at step 495: 380.349\n",
      "Loss at step 496: 377.511\n",
      "Loss at step 497: 374.703\n",
      "Loss at step 498: 371.925\n",
      "Loss at step 499: 369.176\n",
      "Loss at step 500: 366.457\n",
      "Loss at step 501: 363.766\n",
      "Loss at step 502: 361.104\n",
      "Loss at step 503: 358.472\n",
      "Loss at step 504: 355.871\n",
      "Loss at step 505: 353.300\n",
      "Loss at step 506: 350.758\n",
      "Loss at step 507: 348.244\n",
      "Loss at step 508: 345.760\n",
      "Loss at step 509: 343.303\n",
      "Loss at step 510: 340.873\n",
      "Loss at step 511: 338.469\n",
      "Loss at step 512: 336.093\n",
      "Loss at step 513: 333.743\n",
      "Loss at step 514: 331.418\n",
      "Loss at step 515: 329.118\n",
      "Loss at step 516: 326.843\n",
      "Loss at step 517: 324.593\n",
      "Loss at step 518: 322.369\n",
      "Loss at step 519: 320.168\n",
      "Loss at step 520: 317.992\n",
      "Loss at step 521: 315.840\n",
      "Loss at step 522: 313.713\n",
      "Loss at step 523: 311.609\n",
      "Loss at step 524: 309.528\n",
      "Loss at step 525: 307.470\n",
      "Loss at step 526: 305.434\n",
      "Loss at step 527: 303.423\n",
      "Loss at step 528: 301.433\n",
      "Loss at step 529: 299.465\n",
      "Loss at step 530: 297.518\n",
      "Loss at step 531: 295.592\n",
      "Loss at step 532: 293.688\n",
      "Loss at step 533: 291.807\n",
      "Loss at step 534: 289.946\n",
      "Loss at step 535: 288.104\n",
      "Loss at step 536: 286.284\n",
      "Loss at step 537: 284.482\n",
      "Loss at step 538: 282.700\n",
      "Loss at step 539: 280.937\n",
      "Loss at step 540: 279.193\n",
      "Loss at step 541: 277.467\n",
      "Loss at step 542: 275.760\n",
      "Loss at step 543: 274.071\n",
      "Loss at step 544: 272.400\n",
      "Loss at step 545: 270.747\n",
      "Loss at step 546: 269.112\n",
      "Loss at step 547: 267.495\n",
      "Loss at step 548: 265.895\n",
      "Loss at step 549: 264.313\n",
      "Loss at step 550: 262.748\n",
      "Loss at step 551: 261.200\n",
      "Loss at step 552: 259.669\n",
      "Loss at step 553: 258.154\n",
      "Loss at step 554: 256.657\n",
      "Loss at step 555: 255.176\n",
      "Loss at step 556: 253.711\n",
      "Loss at step 557: 252.263\n",
      "Loss at step 558: 250.831\n",
      "Loss at step 559: 249.414\n",
      "Loss at step 560: 248.013\n",
      "Loss at step 561: 246.628\n",
      "Loss at step 562: 245.258\n",
      "Loss at step 563: 243.904\n",
      "Loss at step 564: 242.564\n",
      "Loss at step 565: 241.240\n",
      "Loss at step 566: 239.930\n",
      "Loss at step 567: 238.635\n",
      "Loss at step 568: 237.355\n",
      "Loss at step 569: 236.089\n",
      "Loss at step 570: 234.837\n",
      "Loss at step 571: 233.599\n",
      "Loss at step 572: 232.376\n",
      "Loss at step 573: 231.166\n",
      "Loss at step 574: 229.969\n",
      "Loss at step 575: 228.786\n",
      "Loss at step 576: 227.617\n",
      "Loss at step 577: 226.461\n",
      "Loss at step 578: 225.317\n",
      "Loss at step 579: 224.187\n",
      "Loss at step 580: 223.070\n",
      "Loss at step 581: 221.966\n",
      "Loss at step 582: 220.874\n",
      "Loss at step 583: 219.795\n",
      "Loss at step 584: 218.728\n",
      "Loss at step 585: 217.673\n",
      "Loss at step 586: 216.631\n",
      "Loss at step 587: 215.600\n",
      "Loss at step 588: 214.581\n",
      "Loss at step 589: 213.574\n",
      "Loss at step 590: 212.579\n",
      "Loss at step 591: 211.595\n",
      "Loss at step 592: 210.623\n",
      "Loss at step 593: 209.662\n",
      "Loss at step 594: 208.712\n",
      "Loss at step 595: 207.773\n",
      "Loss at step 596: 206.845\n",
      "Loss at step 597: 205.928\n",
      "Loss at step 598: 205.021\n",
      "Loss at step 599: 204.126\n",
      "Loss at step 600: 203.240\n",
      "Loss at step 601: 202.365\n",
      "Loss at step 602: 201.501\n",
      "Loss at step 603: 200.646\n",
      "Loss at step 604: 199.802\n",
      "Loss at step 605: 198.967\n",
      "Loss at step 606: 198.143\n",
      "Loss at step 607: 197.328\n",
      "Loss at step 608: 196.523\n",
      "Loss at step 609: 195.727\n",
      "Loss at step 610: 194.941\n",
      "Loss at step 611: 194.164\n",
      "Loss at step 612: 193.396\n",
      "Loss at step 613: 192.638\n",
      "Loss at step 614: 191.889\n",
      "Loss at step 615: 191.148\n",
      "Loss at step 616: 190.417\n",
      "Loss at step 617: 189.694\n",
      "Loss at step 618: 188.980\n",
      "Loss at step 619: 188.274\n",
      "Loss at step 620: 187.577\n",
      "Loss at step 621: 186.889\n",
      "Loss at step 622: 186.208\n",
      "Loss at step 623: 185.536\n",
      "Loss at step 624: 184.872\n",
      "Loss at step 625: 184.216\n",
      "Loss at step 626: 183.568\n",
      "Loss at step 627: 182.928\n",
      "Loss at step 628: 182.296\n",
      "Loss at step 629: 181.672\n",
      "Loss at step 630: 181.055\n",
      "Loss at step 631: 180.445\n",
      "Loss at step 632: 179.843\n",
      "Loss at step 633: 179.249\n",
      "Loss at step 634: 178.661\n",
      "Loss at step 635: 178.081\n",
      "Loss at step 636: 177.508\n",
      "Loss at step 637: 176.943\n",
      "Loss at step 638: 176.384\n",
      "Loss at step 639: 175.832\n",
      "Loss at step 640: 175.287\n",
      "Loss at step 641: 174.748\n",
      "Loss at step 642: 174.216\n",
      "Loss at step 643: 173.691\n",
      "Loss at step 644: 173.173\n",
      "Loss at step 645: 172.661\n",
      "Loss at step 646: 172.155\n",
      "Loss at step 647: 171.655\n",
      "Loss at step 648: 171.162\n",
      "Loss at step 649: 170.675\n",
      "Loss at step 650: 170.194\n",
      "Loss at step 651: 169.719\n",
      "Loss at step 652: 169.251\n",
      "Loss at step 653: 168.788\n",
      "Loss at step 654: 168.330\n",
      "Loss at step 655: 167.879\n",
      "Loss at step 656: 167.433\n",
      "Loss at step 657: 166.993\n",
      "Loss at step 658: 166.559\n",
      "Loss at step 659: 166.130\n",
      "Loss at step 660: 165.706\n",
      "Loss at step 661: 165.288\n",
      "Loss at step 662: 164.875\n",
      "Loss at step 663: 164.467\n",
      "Loss at step 664: 164.065\n",
      "Loss at step 665: 163.668\n",
      "Loss at step 666: 163.276\n",
      "Loss at step 667: 162.889\n",
      "Loss at step 668: 162.506\n",
      "Loss at step 669: 162.129\n",
      "Loss at step 670: 161.757\n",
      "Loss at step 671: 161.389\n",
      "Loss at step 672: 161.026\n",
      "Loss at step 673: 160.668\n",
      "Loss at step 674: 160.314\n",
      "Loss at step 675: 159.965\n",
      "Loss at step 676: 159.621\n",
      "Loss at step 677: 159.281\n",
      "Loss at step 678: 158.946\n",
      "Loss at step 679: 158.614\n",
      "Loss at step 680: 158.287\n",
      "Loss at step 681: 157.965\n",
      "Loss at step 682: 157.646\n",
      "Loss at step 683: 157.332\n",
      "Loss at step 684: 157.022\n",
      "Loss at step 685: 156.716\n",
      "Loss at step 686: 156.414\n",
      "Loss at step 687: 156.116\n",
      "Loss at step 688: 155.822\n",
      "Loss at step 689: 155.531\n",
      "Loss at step 690: 155.245\n",
      "Loss at step 691: 154.962\n",
      "Loss at step 692: 154.683\n",
      "Loss at step 693: 154.408\n",
      "Loss at step 694: 154.137\n",
      "Loss at step 695: 153.869\n",
      "Loss at step 696: 153.604\n",
      "Loss at step 697: 153.343\n",
      "Loss at step 698: 153.086\n",
      "Loss at step 699: 152.832\n",
      "Loss at step 700: 152.581\n",
      "Loss at step 701: 152.334\n",
      "Loss at step 702: 152.090\n",
      "Loss at step 703: 151.849\n",
      "Loss at step 704: 151.612\n",
      "Loss at step 705: 151.378\n",
      "Loss at step 706: 151.147\n",
      "Loss at step 707: 150.919\n",
      "Loss at step 708: 150.694\n",
      "Loss at step 709: 150.472\n",
      "Loss at step 710: 150.253\n",
      "Loss at step 711: 150.037\n",
      "Loss at step 712: 149.824\n",
      "Loss at step 713: 149.614\n",
      "Loss at step 714: 149.407\n",
      "Loss at step 715: 149.203\n",
      "Loss at step 716: 149.001\n",
      "Loss at step 717: 148.802\n",
      "Loss at step 718: 148.606\n",
      "Loss at step 719: 148.412\n",
      "Loss at step 720: 148.222\n",
      "Loss at step 721: 148.033\n",
      "Loss at step 722: 147.848\n",
      "Loss at step 723: 147.665\n",
      "Loss at step 724: 147.484\n",
      "Loss at step 725: 147.306\n",
      "Loss at step 726: 147.131\n",
      "Loss at step 727: 146.957\n",
      "Loss at step 728: 146.786\n",
      "Loss at step 729: 146.618\n",
      "Loss at step 730: 146.452\n",
      "Loss at step 731: 146.288\n",
      "Loss at step 732: 146.127\n",
      "Loss at step 733: 145.967\n",
      "Loss at step 734: 145.810\n",
      "Loss at step 735: 145.655\n",
      "Loss at step 736: 145.503\n",
      "Loss at step 737: 145.352\n",
      "Loss at step 738: 145.204\n",
      "Loss at step 739: 145.057\n",
      "Loss at step 740: 144.913\n",
      "Loss at step 741: 144.771\n",
      "Loss at step 742: 144.630\n",
      "Loss at step 743: 144.492\n",
      "Loss at step 744: 144.356\n",
      "Loss at step 745: 144.221\n",
      "Loss at step 746: 144.089\n",
      "Loss at step 747: 143.958\n",
      "Loss at step 748: 143.829\n",
      "Loss at step 749: 143.702\n",
      "Loss at step 750: 143.577\n",
      "Loss at step 751: 143.453\n",
      "Loss at step 752: 143.332\n",
      "Loss at step 753: 143.212\n",
      "Loss at step 754: 143.094\n",
      "Loss at step 755: 142.977\n",
      "Loss at step 756: 142.862\n",
      "Loss at step 757: 142.749\n",
      "Loss at step 758: 142.637\n",
      "Loss at step 759: 142.527\n",
      "Loss at step 760: 142.419\n",
      "Loss at step 761: 142.312\n",
      "Loss at step 762: 142.207\n",
      "Loss at step 763: 142.103\n",
      "Loss at step 764: 142.000\n",
      "Loss at step 765: 141.900\n",
      "Loss at step 766: 141.800\n",
      "Loss at step 767: 141.702\n",
      "Loss at step 768: 141.606\n",
      "Loss at step 769: 141.511\n",
      "Loss at step 770: 141.417\n",
      "Loss at step 771: 141.324\n",
      "Loss at step 772: 141.233\n",
      "Loss at step 773: 141.144\n",
      "Loss at step 774: 141.055\n",
      "Loss at step 775: 140.968\n",
      "Loss at step 776: 140.882\n",
      "Loss at step 777: 140.798\n",
      "Loss at step 778: 140.714\n",
      "Loss at step 779: 140.632\n",
      "Loss at step 780: 140.551\n",
      "Loss at step 781: 140.471\n",
      "Loss at step 782: 140.393\n",
      "Loss at step 783: 140.315\n",
      "Loss at step 784: 140.239\n",
      "Loss at step 785: 140.164\n",
      "Loss at step 786: 140.090\n",
      "Loss at step 787: 140.017\n",
      "Loss at step 788: 139.945\n",
      "Loss at step 789: 139.874\n",
      "Loss at step 790: 139.804\n",
      "Loss at step 791: 139.736\n",
      "Loss at step 792: 139.668\n",
      "Loss at step 793: 139.601\n",
      "Loss at step 794: 139.536\n",
      "Loss at step 795: 139.471\n",
      "Loss at step 796: 139.407\n",
      "Loss at step 797: 139.344\n",
      "Loss at step 798: 139.282\n",
      "Loss at step 799: 139.222\n",
      "Loss at step 800: 139.162\n",
      "Loss at step 801: 139.102\n",
      "Loss at step 802: 139.044\n",
      "Loss at step 803: 138.987\n",
      "Loss at step 804: 138.930\n",
      "Loss at step 805: 138.875\n",
      "Loss at step 806: 138.820\n",
      "Loss at step 807: 138.766\n",
      "Loss at step 808: 138.713\n",
      "Loss at step 809: 138.660\n",
      "Loss at step 810: 138.609\n",
      "Loss at step 811: 138.558\n",
      "Loss at step 812: 138.508\n",
      "Loss at step 813: 138.459\n",
      "Loss at step 814: 138.410\n",
      "Loss at step 815: 138.362\n",
      "Loss at step 816: 138.315\n",
      "Loss at step 817: 138.269\n",
      "Loss at step 818: 138.223\n",
      "Loss at step 819: 138.178\n",
      "Loss at step 820: 138.134\n",
      "Loss at step 821: 138.091\n",
      "Loss at step 822: 138.048\n",
      "Loss at step 823: 138.006\n",
      "Loss at step 824: 137.964\n",
      "Loss at step 825: 137.923\n",
      "Loss at step 826: 137.883\n",
      "Loss at step 827: 137.843\n",
      "Loss at step 828: 137.804\n",
      "Loss at step 829: 137.765\n",
      "Loss at step 830: 137.728\n",
      "Loss at step 831: 137.690\n",
      "Loss at step 832: 137.654\n",
      "Loss at step 833: 137.617\n",
      "Loss at step 834: 137.582\n",
      "Loss at step 835: 137.547\n",
      "Loss at step 836: 137.512\n",
      "Loss at step 837: 137.478\n",
      "Loss at step 838: 137.445\n",
      "Loss at step 839: 137.412\n",
      "Loss at step 840: 137.380\n",
      "Loss at step 841: 137.348\n",
      "Loss at step 842: 137.316\n",
      "Loss at step 843: 137.285\n",
      "Loss at step 844: 137.255\n",
      "Loss at step 845: 137.225\n",
      "Loss at step 846: 137.195\n",
      "Loss at step 847: 137.166\n",
      "Loss at step 848: 137.138\n",
      "Loss at step 849: 137.110\n",
      "Loss at step 850: 137.082\n",
      "Loss at step 851: 137.055\n",
      "Loss at step 852: 137.028\n",
      "Loss at step 853: 137.002\n",
      "Loss at step 854: 136.976\n",
      "Loss at step 855: 136.950\n",
      "Loss at step 856: 136.925\n",
      "Loss at step 857: 136.900\n",
      "Loss at step 858: 136.876\n",
      "Loss at step 859: 136.852\n",
      "Loss at step 860: 136.828\n",
      "Loss at step 861: 136.805\n",
      "Loss at step 862: 136.782\n",
      "Loss at step 863: 136.760\n",
      "Loss at step 864: 136.738\n",
      "Loss at step 865: 136.716\n",
      "Loss at step 866: 136.694\n",
      "Loss at step 867: 136.673\n",
      "Loss at step 868: 136.653\n",
      "Loss at step 869: 136.633\n",
      "Loss at step 870: 136.613\n",
      "Loss at step 871: 136.593\n",
      "Loss at step 872: 136.574\n",
      "Loss at step 873: 136.554\n",
      "Loss at step 874: 136.536\n",
      "Loss at step 875: 136.517\n",
      "Loss at step 876: 136.499\n",
      "Loss at step 877: 136.481\n",
      "Loss at step 878: 136.464\n",
      "Loss at step 879: 136.446\n",
      "Loss at step 880: 136.429\n",
      "Loss at step 881: 136.413\n",
      "Loss at step 882: 136.396\n",
      "Loss at step 883: 136.380\n",
      "Loss at step 884: 136.364\n",
      "Loss at step 885: 136.348\n",
      "Loss at step 886: 136.333\n",
      "Loss at step 887: 136.318\n",
      "Loss at step 888: 136.303\n",
      "Loss at step 889: 136.288\n",
      "Loss at step 890: 136.274\n",
      "Loss at step 891: 136.260\n",
      "Loss at step 892: 136.246\n",
      "Loss at step 893: 136.232\n",
      "Loss at step 894: 136.219\n",
      "Loss at step 895: 136.205\n",
      "Loss at step 896: 136.192\n",
      "Loss at step 897: 136.180\n",
      "Loss at step 898: 136.167\n",
      "Loss at step 899: 136.155\n",
      "Loss at step 900: 136.142\n",
      "Loss at step 901: 136.130\n",
      "Loss at step 902: 136.119\n",
      "Loss at step 903: 136.107\n",
      "Loss at step 904: 136.096\n",
      "Loss at step 905: 136.084\n",
      "Loss at step 906: 136.073\n",
      "Loss at step 907: 136.063\n",
      "Loss at step 908: 136.052\n",
      "Loss at step 909: 136.042\n",
      "Loss at step 910: 136.031\n",
      "Loss at step 911: 136.021\n",
      "Loss at step 912: 136.011\n",
      "Loss at step 913: 136.001\n",
      "Loss at step 914: 135.992\n",
      "Loss at step 915: 135.982\n",
      "Loss at step 916: 135.973\n",
      "Loss at step 917: 135.964\n",
      "Loss at step 918: 135.955\n",
      "Loss at step 919: 135.946\n",
      "Loss at step 920: 135.937\n",
      "Loss at step 921: 135.929\n",
      "Loss at step 922: 135.920\n",
      "Loss at step 923: 135.912\n",
      "Loss at step 924: 135.904\n",
      "Loss at step 925: 135.896\n",
      "Loss at step 926: 135.888\n",
      "Loss at step 927: 135.881\n",
      "Loss at step 928: 135.873\n",
      "Loss at step 929: 135.865\n",
      "Loss at step 930: 135.858\n",
      "Loss at step 931: 135.851\n",
      "Loss at step 932: 135.844\n",
      "Loss at step 933: 135.837\n",
      "Loss at step 934: 135.830\n",
      "Loss at step 935: 135.823\n",
      "Loss at step 936: 135.817\n",
      "Loss at step 937: 135.810\n",
      "Loss at step 938: 135.804\n",
      "Loss at step 939: 135.798\n",
      "Loss at step 940: 135.792\n",
      "Loss at step 941: 135.786\n",
      "Loss at step 942: 135.780\n",
      "Loss at step 943: 135.774\n",
      "Loss at step 944: 135.768\n",
      "Loss at step 945: 135.762\n",
      "Loss at step 946: 135.757\n",
      "Loss at step 947: 135.751\n",
      "Loss at step 948: 135.746\n",
      "Loss at step 949: 135.741\n",
      "Loss at step 950: 135.736\n",
      "Loss at step 951: 135.731\n",
      "Loss at step 952: 135.726\n",
      "Loss at step 953: 135.721\n",
      "Loss at step 954: 135.716\n",
      "Loss at step 955: 135.711\n",
      "Loss at step 956: 135.706\n",
      "Loss at step 957: 135.702\n",
      "Loss at step 958: 135.697\n",
      "Loss at step 959: 135.693\n",
      "Loss at step 960: 135.689\n",
      "Loss at step 961: 135.684\n",
      "Loss at step 962: 135.680\n",
      "Loss at step 963: 135.676\n",
      "Loss at step 964: 135.672\n",
      "Loss at step 965: 135.668\n",
      "Loss at step 966: 135.664\n",
      "Loss at step 967: 135.660\n",
      "Loss at step 968: 135.657\n",
      "Loss at step 969: 135.653\n",
      "Loss at step 970: 135.649\n",
      "Loss at step 971: 135.646\n",
      "Loss at step 972: 135.642\n",
      "Loss at step 973: 135.639\n",
      "Loss at step 974: 135.635\n",
      "Loss at step 975: 135.632\n",
      "Loss at step 976: 135.629\n",
      "Loss at step 977: 135.626\n",
      "Loss at step 978: 135.623\n",
      "Loss at step 979: 135.619\n",
      "Loss at step 980: 135.616\n",
      "Loss at step 981: 135.613\n",
      "Loss at step 982: 135.611\n",
      "Loss at step 983: 135.608\n",
      "Loss at step 984: 135.605\n",
      "Loss at step 985: 135.602\n",
      "Loss at step 986: 135.599\n",
      "Loss at step 987: 135.597\n",
      "Loss at step 988: 135.594\n",
      "Loss at step 989: 135.592\n",
      "Loss at step 990: 135.589\n",
      "Loss at step 991: 135.587\n",
      "Loss at step 992: 135.584\n",
      "Loss at step 993: 135.582\n",
      "Loss at step 994: 135.579\n",
      "Loss at step 995: 135.577\n",
      "Loss at step 996: 135.575\n",
      "Loss at step 997: 135.573\n",
      "Loss at step 998: 135.570\n",
      "Loss at step 999: 135.568\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "# nepochs = number of times to run through the data.\n",
    "nepochs = 1000  # Could probably do better with more epochs, but should suffice\n",
    "\n",
    "for i in range(nepochs):\n",
    "  grads = grad(input_train_data, output_train_data, nnet)\n",
    "  optimizer.apply_gradients(zip(grads, nnet.weights))  # SGD-type update (w/ Adam)\n",
    "  print(\"Loss at step {:03d}: {:.3f}\".format(\n",
    "      i, temp_loss(input_train_data, output_train_data, nnet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20a035ab380>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMdNJREFUeJzt3Qt4VNW5//E34RJASDDckkCgAZWrwvECooigCKi1YuHvpSJwpPSIilIUkT7e0HpAfax4AezpURBbQFFRRJs+igUEQxUsCEU5giAoCViVDKBcs//Pu4aJmVwgE/as2Xv29/M84zh7Npk9WTOZ36y91rtSHMdxBAAAwJJUWw8EAACgCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArKotHlNSUiI7duyQRo0aSUpKSqIPBwAAVIPWLN2zZ4/k5ORIamqqv8KHBo/c3NxEHwYAAKiB7du3S6tWrfwVPrTHI3Lw6enpiT4cAABQDaFQyHQeRD7HXQsfM2bMMJetW7ea2507d5b77rtPLr30UnN7//79cscdd8i8efPkwIEDMmDAAJk+fbq0aNGi2o8ROdWiwYPwAQCAv1RnyERMA061G2XKlCmyevVqWbVqlVx00UVy5ZVXyr/+9S9z/29/+1t58803Zf78+bJ06VJzCuWXv/xlzZ8BAABIOiknuqptZmamPPbYYzJkyBBp1qyZzJkzx/y/+uyzz6Rjx45SUFAg5557brW7bTIyMqS4uJieDwAAfCKWz+8aT7U9cuSIOb2yb98+6dmzp+kNOXTokPTr1690nw4dOkjr1q1N+KiKnp7RAy57AQAAySvmAafr1q0zYUPHdzRs2FAWLFggnTp1kjVr1kjdunWlcePGUfvreI+ioqIqf97kyZNl0qRJMR2DdtYcPnzYBCD4U61ataR27dpMpwaAAIo5fLRv394EDe1WeeWVV2T48OFmfEdNTZw4UcaNG1dhtGxVDh48KIWFhfLDDz/U+DHhDQ0aNJDs7GwTWgEAwRFz+NAPilNOOcX8/1lnnSUfffSRPPnkk3LNNdeYYLB79+6o3o+dO3dKVlZWlT8vLS3NXKpbgGzLli3mW7MWMdFj4Zuz/2jPlb5WvvnmG9Oep5566nEL0gAAkscJ1/nQQKDjNjSI1KlTRxYvXiyDBw82923cuFG2bdtmTtO4QT+w9PG0Z0S/NcO/6tevb14vX375pWnXevXqJfqQAABeDB96ikRreuggUi2hqjNblixZIn/729/MCNeRI0eaUyg6A0ZHuo4ZM8YEj+rOdKkuviUnB9oRAIIppvCxa9cuGTZsmBlzoWHjjDPOMMHjkksuMfc/8cQT5gNFez7KFhkDAARcyRGRLz8Q2btTpGELkTbniaTWSvRRwa91PmzOE9YZNjpGIC8vj276JEB7AgGxYaFI/gSR0I6ftqXniAx8RKTTLxJ5ZPBbnQ9404gRI2TQoEGlt/v06SNjx461fhx6Ok4HA+sAZAABDx4vD4sOHipUGN6u9yNwCB8WQ4F+GOslMmPowQcfNPVK4um1116Thx56qFr7EhgAuH6qRXs8pLIO9qPb8u8O74dA8dyqtrYcKXHkwy3fya49+6V5o3rSPS9TaqXGd9ruwIEDZebMmWY8zNtvvy233HKLmfGhA3nL0tkfbtW+0MG/AJAQOsajfI9HFEck9HV4v7wLLB4YEi2QPR/56wul1yPvyXV/Wim3z1tjrvW2bo8nrWeiNU/atGkjo0ePNqXoFy5cWHqq5OGHHzb1S7SQm9q+fbtcffXVpm6KhghdxC+yorDSCq86u0jvb9Kkidx1112mhkZZ5U+7aPCZMGGCma6sx6M9MM8995z5uX379jX7nHzyyaYHRI9L6fRmrUSrYzN0imzXrl1NgbmyNEyddtpp5n79OWWPE0BA6eBSN/dD0ghc+NCAMfrPH0th8f6o7UXF+832eAeQsvSDWns5lNZH0boo77zzjixatMisk6OzhRo1aiTvv/++rFixwpSz196TyL95/PHHZdasWfL888/L8uXL5bvvvjPl7o9FZyvNnTtXnnrqKfn000/lj3/8o/m5GkZeffVVs48eh85o0uJxSoPH7Nmz5dlnnzUrGOvqxUOHDi2tbKshSVcvvuKKK0z121//+tdy9913x/m3B8DzdFaLm/shadQO2qmWSW9uqPLso5500fsv6ZQV11Mw2juhYUOnKWstFK30edJJJ8n//u//lp5u+fOf/2x6HHRbpIqrnrLRXg4dm9G/f3+ZOnWqOWWjH/xKw4H+zKr83//9n7z88ssm4EQWAGzbtm2FUzTNmzcvrVKrPSX//d//Le+++25psTj9Nxp2NLhceOGFMmPGDGnXrp0JQ0p7bnQNoEceeSROv0EAvqDTaXVWiw4urfQvb0r4ft0PgRKo8KFjPMr3eJSlbw29X/fr2a6J64+vPRray6C9GhosfvWrX8kDDzxgxn6cfvrpUeM81q5dK5s2bTI9H+Wnp27evNlMZdLeiR49epTepwu1nX322RVOvURor4SWptfAUF16DLqOTqSWS4T2vvzHf/yH+X/tQSl7HMqtqrYAfEzreOh0Wp3VYr7elf3bdPQL3sAp1PsIoECFDx1c6uZ+sdKxENpLoCFDx3ZoWIjQno+y9u7da0rW/+Uvf6nwc5o1a1bj0zyx0uNQb731lrRs2TLqvuquyQMgwLSOx9Wzq6jzMYU6HwEVqPChs1rc3C9WGjAii/Idz5lnnikvvfSSOQVSVbEWXRH2H//4h/Tu3dvc1mm7q1evNv+2Mtq7oj0uOlYjctqlrEjPiw5kjejUqZMJGbpGT1U9Jh07djQDZ8tauXJltZ4ngADQgNHhciqcIpgDTnU6bXZGvUhnXwW6Xe/X/RLt+uuvl6ZNm5oZLjrgVCuB6liP2267Tb766iuzz+233y5TpkyR119/XT777DO5+eabj1mj42c/+5kMHz5cbrzxRvNvIj9Tx4EonYWj40v09JCOQ9FeDz3tc+edd5pBpi+88II55fPxxx/L008/bW6rm266ST7//HMZP368Gayqa/7oQFgAKKVBQ6fTnj4kfE3wSAytqbLlfZF1r4SvE1RjJVDhQweR3n9FJ/P/5QNI5LbeH+96H9Whq/YuW7bMLOKnA0q1d0EX7tMxH5GekDvuuENuuOEGEyh0jIUGhauuuuqYP1dP+wwZMsQElQ4dOsioUaNk37595j49rTJp0iQzU6VFixZy6623mu1apOzee+81s170OHTGjZ6G0am3So9RZ8pooNFpuDrwVQepAgA8ZMNCkaldRF74ucirI8PXejsBVWYDubaLTqfVWS1lB59qj4cGj4Fdsk/o+FF9rO0CAJbL3FeYdXT0y7aOyznB8TexrO0SqDEfERowdDqt7QqnAAB4r8x9SrjMvY7LsXQ6LJDhQ2nQiMd0WgAAPOVL75W5D9SYDwAAAmev98rcEz4AAEhmDb1X5p7wAQBAEMrcyzEKTaS3tFrmnvABAEAQytwfq9CE5TL3hA8AAIJS5j69XDkJ7RFxYZptrAI72wUAgEDp5J0y9/R8wJRU1+qkAIAkl+qNMveED8sKCgrMsvaXX355TP9O12WZOnVq3I4LAABbghs+ErS4znPPPSdjxowx67bs2HGsoi8AACSnYIaPBC2uo6vEvvTSSzJ69GjT81F+5dc333xTzjnnHLPOia5oG1kkrk+fPvLll1+alWX1FIle1AMPPCDdunWL+hnaO6K9JBEfffSRXHLJJebnac39Cy+80KxKCwBAoqQGdnGd8qVmQ4Xh7XEMILp0va4k2759exk6dKg8//zzElnXT1eJ1bBx2WWXyT//+U9ZvHixdO/e3dz32muvSatWreTBBx+UwsJCc6muPXv2mFVvly9fLitXrpRTTz3VPIZuBwAgEYI12yXBi+voKRcNHUqXpdeV/5YuXWp6Nh5++GG59tprzZL2Ebo8vcrMzDTjRBo1aiRZWVkxPeZFF10Udft//ud/pHHjxuZxf/7zn7vyvAAAiEWwej5iWVzHZRs3bpQPP/xQrrvuOnO7du3acs0115hAotasWSMXX3yx64+7c+dOGTVqlOnx0NMuusyxnv7Ztm2b648FAEB1BKvnI4GL62jIOHz4sOTkaInbMD3lkpaWJs8884zUr18/5p+Zmppaetom4tChQ1G39ZTLt99+K08++aS0adPGPF7Pnj3l4MGDJ/BsAACouWD1fCRocR0NHbNnz5bHH3/c9HBELmvXrjVhZO7cuXLGGWeYcR5VqVu3rhw5Ej0jp1mzZlJUVBQVQPTnlrVixQq57bbbzDiPzp07m/Dx73//29XnBwBALGoHcnEdHVxa6bgPXVwnx/XFdRYtWiTff/+9jBw50pz6KGvw4MGmV+Sxxx4zp13atWtnxn5oYHn77bdlwoQJpXU+dHqu3qcBQmev6FiRb775Rh599FEZMmSI5Ofny1//+ldzaiVCT7e8+OKLcvbZZ0soFJLx48fXqJcFAAC3BKvnI0GL62i46NevX4XgEQkfq1atMoNK58+fLwsXLjTTZ3WgqI4RidCZLlu3bjXhRHs8VMeOHWX69Okybdo0MzhV97/zzjsrPLYGnzPPPFNuuOEG0wvSvHlzV58fAACxSHHKDxpIMP12rh/SOhOk7Dd4tX//ftmyZYvk5eWZWhg1ptNpddZL2cGnupywBg/Li+sEmWvtCQDw9Od3sE+7eHBxHQAAgiaY4aPs4joAAMCqYI35AAAACRfcng8AACLVrzkNbxXhAwAQXJVOQMgJz4xkAkLc+PK0i8cm6KCGaEcAQV1oNOh8FT7q1Kljrn/44YdEHwpcEGnHSLsCgHcWGpXwQqO6H4J92kVXdtUVWXft2mVuN2jQQFJSyhcLgx96PDR4aDtqe2q7AoBnFxplZmSww4eKLCkfCSDwLw0ekfYEgKAsNAofhg/t6cjOzjYlwsuv4Ar/0FMt9HgACNpCo/Bp+IjQDy4+vAAAflpoFD4ccAoAgJ8XGkUY4QMAEExax+Pq2SLp2dHbtcdDt1PnI258e9oFAIATxkKjCUH4AAAEGwuNevu0y+TJk+Wcc86RRo0amdkmgwYNko0bN0bt06dPHzMjpezlpptucvu4AQBAEMLH0qVL5ZZbbpGVK1fKO++8Y6a69u/fX/bt2xe136hRo6SwsLD08uijj7p93AAAIAinXfLz86Nuz5o1y/SArF69Wnr37l26XSuPUjwKAAC4PtuluLjYXGdmZkZt/8tf/iJNmzaVLl26yMSJE4+5FsuBAwckFApFXQAAQPKq8YDTkpISGTt2rJx//vkmZET86le/kjZt2khOTo588sknMmHCBDMu5LXXXqtyHMmkSZNqehgAAMBnUpwarms+evRo+etf/yrLly+XVq1aVbnfe++9JxdffLFs2rRJ2rVrV2nPh14itOcjNzfX9Kqkp6fX5NAAAIBl+vmdkZFRrc/vGvV83HrrrbJo0SJZtmzZMYOH6tGjh7muKnykpaWZCwAACIaYwod2kowZM0YWLFggS5Yskby8vOP+mzVr1phrXQwOAAAgpvCh02znzJkjb7zxhqn1UVRUZLZrN0v9+vVl8+bN5v7LLrtMmjRpYsZ8/Pa3vzUzYc4444x4PQcAAJCsYz60YFhlZs6cKSNGjJDt27fL0KFDZf369ab2h47duOqqq+See+6p9viNWM4ZAQCAJB/zcbycomFDC5EBAABUhVVtAQCAVYQPAABgFeEDAABYRfgAAAD+KK/uOyVHRL78QGTvTpGGLUTanCeSWivRRwUAQOAEI3xsWCiSP0EktOOnbek5IgMfEen0i0QeGQAAgZMaiODx8rDo4KFCheHtej8AALAmNelPtWiPh1RWn+Totvy7w/sBAAArkjt86BiP8j0eURyR0Nfh/QAAgBXJHT50cKmb+wEAgBOW3OFDZ7W4uR8AADhhyR0+dDqtzmqRyhfEM9vTW4b3AwAAViR3+NA6Hjqd1igfQI7eHjiFeh8AAFiU3OFDaR2Pq2eLpGdHb9ceEd1OnQ8AAKwKRpExDRgdLqfCKQAAHhCM8KE0aORdkOijAAAg8JL/tAsAAPAUwgcAALCK8AEAAKwifAAAAKsIHwAAwKrgzHYBAK/RFbUpAZB8aNfjInwAQCJsWCiSPyF65W0tfqhVmSl+6F+0a7Vw2gUAEvEB9fKw6A8oFSoMb9f74T+0a7URPgDAdpe8fjMWp5I7j27Lvzu8H/yDdo0J4QMAbNKxAOW/GUdxREJfh/eDf9CuMSF8AIBNOgjRzf3gDbRrTAgfAGCTzn5wcz94A+0aE8IHANik0y519oOkVLFDikh6y/B+8A/aNSaEDwCwSes96LRLo/wH1dHbA6dQF8JvaNeYED4AwDat93D1bJH07Ojt+s1Zt1MPwp9o12pLcRynsnlBCRMKhSQjI0OKi4slPT090YcDAPFDJczkFNB2DcXw+U2FUwBIFP1Ayrsg0UcBt9Gux8VpFwAAYBXhAwAAWEX4AAAAVhE+AACAVQw4hb8FdFQ5APgZ4QP+pctT6yqSZRdz0vn0WuiH+fQA4FmcdoF/g8fLwyquIhkqDG/X+wEAnkT4gD9PtWiPhy5RXcHRbfl3h/cDAHgO4QP+o2M8yvd4RHFEQl+H9wMAeA7hA/6jg0vd3A8AYBXhA/6js1rc3A8AYBWzXXBMR0oc+XDLd7Jrz35p3qiedM/LlFqp5ZeLtkyn0+qsFh1cWum4j5Tw/bofAMBzCB+oUv76Qpn05gYpLN5fui07o57cf0UnGdil3JLRNmkdD51Oq7NaNGhEBZCjwWjgFOp9AEAynHaZPHmynHPOOdKoUSNp3ry5DBo0SDZu3Bi1z/79++WWW26RJk2aSMOGDWXw4MGycyfn3v0YPEb/+eOo4KGKiveb7Xp/Qmkdj6tni6SXC0Ha46Hba1LnQ2fHbHlfZN0r4Wtmy8BveA3DJ1Icx6ms37pSAwcOlGuvvdYEkMOHD8vvfvc7Wb9+vWzYsEFOOukks8/o0aPlrbfeklmzZklGRobceuutkpqaKitWrKjWY4RCIfPviouLJT09vebPDCd0qqXXI+9VCB5l+xayMurJ8gkXJf4UjFsVTilYBr/jNYwEi+XzO6bwUd4333xjekCWLl0qvXv3Ng/YrFkzmTNnjgwZMsTs89lnn0nHjh2loKBAzj33XFcPHvFRsPlbue5PK4+739xR50rPdk0kaQqWVRg/cjRY1bQnBbD8GnbEibxqDb1lbvMahgWxfH6f0GwXfQCVmZlprlevXi2HDh2Sfv36le7ToUMHad26tQkf8AcdXOrmfp5GwTIkyWu4fPBQKWYrr2F4T43DR0lJiYwdO1bOP/986dKli9lWVFQkdevWlcaNG0ft26JFC3NfZQ4cOGDSUtkLEktntbi5n6dRsAxJ8hqu6gSoBhBew0ia8KGDSnW8x7x5807oAHQQq3bTRC65ubkn9PNw4nQ6rc5qqfqPWXjWi+7nexQsg8+V7ClydT94bwxeweZv5Y01X5trvR3YqbY6iHTRokWybNkyadWqVen2rKwsOXjwoOzevTuq90Nnu+h9lZk4caKMGzeu9Lb2fAQtgHitloY+tk6n1VktVUxkNfcnfLCpGyhYBp/7dE8D6ezifvCOfK+WO7AdPnRs6pgxY2TBggWyZMkSycvLi7r/rLPOkjp16sjixYvNFFulU3G3bdsmPXv2rPRnpqWlmUtQefXFpY89Y+iZFY4tywPH5qqAFizzWuBFzW1qcLqc7GRKlnwnlTWhflEukiZmP8KH/8odOOW2R8od6N9nP/8drh3rqRadyfLGG2+YWh+RcRx6uqR+/frmeuTIkaYnQweh6mhXDSsaPKoz0yWoL64UKZFzUz+T5rJbdklj+ai4gydeXPrYl3TKSu4PqQAWLPNq4EXNNE8/SSYdGiYz6kw1QaPs2zPSQz/p0A0yIj1cDgHeD/dHShzzHq1qGLz+JL1f/z779e9xTFNtU1Iqf5IzZ86UESNGlBYZu+OOO2Tu3LlmMOmAAQNk+vTpVZ52CepU20gtjTP2LJP768yWnJTvSu/b4WTKg4eGydpGvb1RSyOwNRJahoNHEk1RrOrbVOQVlujAi5r/Lem6Z5ncV+FvSRN58NAN/C3xWbgv8Gm5g1g+v2M+7XI89erVk2nTppmLl3itm1mPRYOHflspT7tPp9eZKqP36H7dPPXiSloaMDpc7k7BMo8KwrepIPppjNZ+eefA2XJO2V7Ukg5SIqkyI1nGaAXkVMmuAJQ7CMTaLl7sZt4V2md6PFT5vwl6W7tL76/zonwUGikihA8rNGjkXSDJSgNvVVVrlf7B1Pt1PwKvv5Qdo7WyuJNn/s4FiZvhvnkAyh0kffjw6qCdU35YF9U9Wp6+NnPkW7OfSGurx4bkFIRvU0HrQQ3cGK2AhPvuR8sd6OdUFcPgzeB/P5c7SOrw4eVu5o6NfnB1P99way0WxCwI36aC1oNanv4do9fK/+G+VgDKHZxQefVkSqK2pTbKcnU/3wzqnNpF5IWfi7w6Mnytt3V7DSVrAZ54CFTxuKCt8uwDyf5edTvcDzx6Kk17OMrS28kwMDypez7i1s3sxrf3o/UlnFBhuPxxOWZBqGSqL1HV4m1aX0O312DhKz98E/WSIHybCloPql8E4b0aj1MlA5P4VFpS93zEpZt5w0Jxyn17d2ry7f1ofYnwh0D0C6l0Jcoa1pfw3DeMOCze5odvop5rhwB8mwpaD6of+OG96ma4VxUX+Kt5uK919FTald1amutkCB5J3/PhehLV4FHZstVaG+LlYZIS67d33ffq2ZJSrr6E6fGoYX0JT37DiGXxtmrMNvHDN1G328HNgY7J/G0qHhioW3N+eK+6KTCVoV2Q1OHD1W7mkiPy45vjJc1xKk6NNZUEHdn/5nipr7UiYumtcLG+hFdn9ri9eJvXp4y63Q7xCJQMTKw+BurWXLzeq8w68r+kDh9uJtEjW1dI/R+LKvanHaWvK71f96vVtrf1+hKe/obh8uJtXv4m6nY7eDZQBkgQpj3GSzzeq57s3S2HcB/wMR8R+oLU0sJaivbJa7uZa70dywt18xebXd0vUOelI4u3HWuehZYyr+bgWi9/E3WzHY4XZJTe74WxJMksXufyg8Dt92pQxo8EQSDChxuDdnY5jV3dz21e7g0oXbztWH++Yxhc6+Upo262g6cDZcAwULdm3HyvEsaTS9KfdnFLrZ+dLzuWH3/Zat0vEbzcG1B2cG3FxdtiH1zr5SmjbraDpwNlAHEuP3Zuvle9PtYLsQlMz8eJ6t6umTxV59fm/8sH68jtp+qMNPslgpd7A0ppwBi7XmT4IpHBz4Wvx66r0awer34TdbMdPB8oAyhZpz3Gk1vvVcJ4cqHno5r0j0yfQTfKzXMOhpetlp+6urXHQ5etHvT/bkzYHyMv9wbEa/E2L34TdbMdGOiIZOHGe5UwnlxSHMfx1AmyUCgkGRkZUlxcLOnp6eI1OqDpoYXrJHfv2tJlq7c37Cr3/uJ0T5z39cNI8CBwqx0iA+ykiiDDeAMEhY7l6PXIe8cN4zqZIOFfsgIqFMPnN+GjBrw8x9wPxxcUbrUDgRIII4x7G+EDSDIESiCMMO5dsXx+M+YD8IFaUiI9UzeI1NopkqrF2LQmSuxVcAG/8+JYL8SO8AF4nS5aWOkU5UdqNFMI8DsqiPofU20BrwePl4dVXJgvVBjeHutqygDgAYQPwKtKjoR7PI5V0zH/7vB+AOAjhA/Aq3Sl4/I9HlEckdDX4f0AwEcIH4BX7d3p7n4A4BGED8CrGrZwdz8A8AjCB+BVbc4Lz2o51kox6S3D+wGAjxA+AK/SdXB0Oq1RPoAcva0rAut+AOAjhA/Ay7SOx9WzRdLLVW7UHhHdTp0PAD5EkTHA6zRgdLg8PKtFB5fqGA891UKPBwCfInwAfqBBI++CRB8FALiC0y4AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArKLOBwAAXlVyJCkLDBI+AADwog0LRfIniIR2RC+toGs++XxpBU67APH8xrLlfZF1r4Sv9bZXePnYAIgJHi8Piw4eKlQY3q73+xg9H0DQvrF4+dgAiPkyoO9RcSq5U7eliOTfHV7zyaenYOj5AIL0jcXLxwYgTMd4lH+PRnFEQl+H9/Mpwgdg9RuLhL+xJOI0h5ePDcBPdHCpm/t5EOEDCMo3Fi8fG4Cf6KwWN/fzIMIHEJRvLF4+NgA/0em0Og5Lx3ZUKkUkvWV4P58ifABB+cbi5WMD8BMdRKoDwI3yAeTo7YFTfDvYVBE+gKB8Y/HysQGIpjPPrp4tkp4dvV3fw7rd5zPTmGoLxOMbi84cMR/yjne+sXj52ABUpAFDp9MmYYXTmHs+li1bJldccYXk5ORISkqKvP7661H3jxgxwmwvexk4cKCbxwx4m5e/sXj52ABUpEEj7wKR04eEr5MgeNSo52Pfvn3StWtXufHGG+WXv/xlpfto2Jg5c2bp7bS0tBM7SsBvvPyNxcvHBiAQYg4fl156qbkci4aNrKysEzkuIHm+sXiRl48NQNKLy4DTJUuWSPPmzaV9+/YyevRo+fbbb6vc98CBAxIKhaIuAAKMdWeApOf6gFM95aKnY/Ly8mTz5s3yu9/9zvSUFBQUSK1aFbt1J0+eLJMmTXL7MAD4EevOAIGQ4jiOU+N/nJIiCxYskEGDBlW5zxdffCHt2rWTd999Vy6++OJKez70EqE9H7m5uVJcXCzp6ek1PTQAfhNZd6ZC+fejM3EYEAt4mn5+Z2RkVOvzO+51Ptq2bStNmzaVTZs2VTk+RA+y7AVAwLDuDBAocQ8fX331lRnzkZ1dbmofAESw7gwQKDGP+di7d29UL8aWLVtkzZo1kpmZaS46fmPw4MFmtouO+bjrrrvklFNOkQEDBrh97ACSBevOAIESc/hYtWqV9O3bt/T2uHHjzPXw4cNlxowZ8sknn8gLL7wgu3fvNoXI+vfvLw899BC1PvxKu7mpB4F4Y90ZIFBiDh99+vSRY41R/dvf/naixwSvYOYBbK87EyqsYtyHrjuTw7ozQJJgYTkce+ZB+fPw+uGg2/V+wC0BWMUTwE8IH6iImQdIBNadAQKDVW1xYjMPKNENN7HuDBAIhA9UxMwDJHJgMuvOAEmP8IGKmHmAWDAwGUCMGPOBqmceVBj4V3bmQUtmHoCByQBqhPCBiph5gOpgYDKAGiJ8oHLMPMDxUBIdQA0x5gNVY+YBjoWByQBqiPCBY2PmAarCwGQANcRpFwA1w8BkADVE+ABQMwxMBlBDhA8ANcfAZCSCzqDa8r7IulfC18yo8h3GfAA4MQxMhk0UtUsKhA8AJ46BybBZ1K58bZlIUTt623yD8JGM3FxnA8BPeG95uKhdSrionfbC0SaeR/hINnRJAvHBeyuxWG07qTDgNJmwzgYQH7y3Eo+idkmF8JEsWGcDiA/eW95AUbukQvhIFqyzAcQH7y1voKhdUiF8JAu6JIH44L3lDRS1SyqEj2RBlyQQH7y3vIOidkmD2S7J1iWpA+AqPTetXZI5dEkCseK95S0UtUsK9HwkC7okgfjgveXdonanDwlf87v3HcJHMqFLEogP3luAq1Icx6msHzFhQqGQZGRkSHFxsaSnpyf6cPyJKoxAfPDeAlz5/GbMRzJinQ0gPnhvAa7gtAsAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADA2+Fj2bJlcsUVV0hOTo6kpKTI66+/HnW/4zhy3333SXZ2ttSvX1/69esnn3/+uZvHDAAAghQ+9u3bJ127dpVp06ZVev+jjz4qTz31lDz77LPyj3/8Q0466SQZMGCA7N+/343jBQAAPlc71n9w6aWXmktltNdj6tSpcs8998iVV15pts2ePVtatGhhekiuvfbaEz9iAADga66O+diyZYsUFRWZUy0RGRkZ0qNHDykoKKj03xw4cEBCoVDUBQAAJC9Xw4cGD6U9HWXp7ch95U2ePNkElMglNzfXzUMCAAAek/DZLhMnTpTi4uLSy/bt2xN9SAAAwC/hIysry1zv3LkzarvejtxXXlpamqSnp0ddAABA8nI1fOTl5ZmQsXjx4tJtOoZDZ7307NnTzYcCAABBme2yd+9e2bRpU9Qg0zVr1khmZqa0bt1axo4dK7///e/l1FNPNWHk3nvvNTVBBg0a5PaxAwCAIISPVatWSd++fUtvjxs3zlwPHz5cZs2aJXfddZepBfKb3/xGdu/eLb169ZL8/HypV6+eu0cOAAB8KcXR4hweoqdpdNaLDj5l/AcAAP4Qy+d3wme7AACAYCF8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwN/h44EHHpCUlJSoS4cOHdx+GAAA4FO14/FDO3fuLO++++5PD1I7Lg8DAAB8KC6pQMNGVlZWPH40AADwubiM+fj8888lJydH2rZtK9dff71s27atyn0PHDggoVAo6gIAAJKX6+GjR48eMmvWLMnPz5cZM2bIli1b5IILLpA9e/ZUuv/kyZMlIyOj9JKbm+v2IQEAAA9JcRzHiecD7N69W9q0aSN/+MMfZOTIkZX2fOglQns+NIAUFxdLenp6PA8NAAC4RD+/tROhOp/fcR8J2rhxYznttNNk06ZNld6flpZmLgAAIBjiXudj7969snnzZsnOzo73QwEAgCCGjzvvvFOWLl0qW7dulQ8++ECuuuoqqVWrllx33XVuPxQAAPAh10+7fPXVVyZofPvtt9KsWTPp1auXrFy50vw/AACA6+Fj3rx5bv9IAACQRFjbBQAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhV2+7DIeiOlDjy4ZbvZNee/dK8UT3pnpcptVJTxAu8fGxe5+bvLkjtEKTn6mVBaocjHnmucQsf06ZNk8cee0yKioqka9eu8vTTT0v37t3j9XDwgfz1hTLpzQ1SWLy/dFt2Rj25/4pOMrBLNsfmU27+7oLUDkF6rl4WpHbI99BzTXEcx3H7h7700ksybNgwefbZZ6VHjx4ydepUmT9/vmzcuFGaN29+zH8bCoUkIyNDiouLJT093e1DQwJf9KP//LGUf7FF8vaMoWcm7I3u5WPzOjd/d0FqhyA9Vy8LUjvkW3iusXx+x2XMxx/+8AcZNWqU/Od//qd06tTJhJAGDRrI888/H4+Hgw+6+TRtV5ZyI9v0ft3PNi8fm9e5+bsLUjsE6bl6WZDa4YgHn6vr4ePgwYOyevVq6dev308PkppqbhcUFFTY/8CBAyYtlb0guej5xbLdfOXpy13v1/1s8/KxeZ2bv7sgtUOQnquXBakdPvTgc3U9fPz73/+WI0eOSIsWLaK2620d/1He5MmTTTdN5JKbm+v2ISHBdGCTm/sF5di8zs3fXZDaIUjP1cuC1A67PPhcEz7VduLEieb8UOSyffv2RB8SXKYjqt3cLyjH5nVu/u6C1A5Beq5eFqR2aO7B5+p6+GjatKnUqlVLdu7cGbVdb2dlZVXYPy0tzQxMKXtBctGpXDqiuqrJXLpd79f9bPPysXmdm7+7ILVDkJ6rlwWpHbp78Lm6Hj7q1q0rZ511lixevLh0W0lJibnds2dPtx8OPqBzyHUqlyr/4o/c1vsTMdfcy8fmdW7+7oLUDkF6rl4WpHao5cHnGpfTLuPGjZM//elP8sILL8inn34qo0ePln379pnZLwgmncKlU7myMqK79fR2oqezefnYvM7N312Q2iFIz9XLgtQOAz32XONS50M988wzpUXGunXrJk899ZSp+XE81PlIbl6prue3Y/M6KpzWTJCeq5cFqR2OxPG5xvL5HbfwUVOEDwAA/CfhRcYAAACqQvgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWFVbPCZScFUrpQEAAH+IfG5Xp3C658LHnj17zHVubm6iDwUAANTgc1zLrPtqbZeSkhLZsWOHNGrUSFJSUlxPZRpqtm/fzroxCUQ7eAPt4A20gzfQDidO44QGj5ycHElNTfVXz4cecKtWreL6GPrC4sWVeLSDN9AO3kA7eAPtcGKO1+MRwYBTAABgFeEDAABYFajwkZaWJvfff7+5RuLQDt5AO3gD7eANtINdnhtwCgAAklugej4AAEDiET4AAIBVhA8AAGAV4QMAAFjlq/AxefJkOeecc0z10+bNm8ugQYNk48aNUfvs379fbrnlFmnSpIk0bNhQBg8eLDt37ozaZ9u2bXL55ZdLgwYNzM8ZP368HD58OGqfJUuWyJlnnmlGPp9yyikya9YsK8/RD2y1g7aBVrktfykqKrL2XIPQDrfddpucddZZ5rXerVu3Sh/rk08+kQsuuEDq1atnqkA++uijcX1ufmKrHbZu3Vrp+2HlypVxf45BaYe1a9fKddddZ17j9evXl44dO8qTTz5Z4bH4fHCB4yMDBgxwZs6c6axfv95Zs2aNc9lllzmtW7d29u7dW7rPTTfd5OTm5jqLFy92Vq1a5Zx77rnOeeedV3r/4cOHnS5dujj9+vVz/vnPfzpvv/2207RpU2fixIml+3zxxRdOgwYNnHHjxjkbNmxwnn76aadWrVpOfn6+9ecc5Hb4+9//rjOxnI0bNzqFhYWllyNHjlh/zsnaDmrMmDHOM88849xwww1O165dKzxOcXGx06JFC+f66683jzV37lynfv36zh//+Ecrz9PrbLXDli1bzPvh3XffjXo/HDx40MrzDEI7PPfcc85tt93mLFmyxNm8ebPz4osvmte6fgZE8PngDl+Fj/J27dpl3oxLly41t3fv3u3UqVPHmT9/fuk+n376qdmnoKDA3NYPudTUVKeoqKh0nxkzZjjp6enOgQMHzO277rrL6dy5c9RjXXPNNebFDXvtEAkf33//vfXnFJR2KOv++++v9ENv+vTpzsknn1zaLmrChAlO+/bt4/Zc/Cxe7RAJHxrWEf92iLj55pudvn37lt7m88EdvjrtUl5xcbG5zszMNNerV6+WQ4cOSb9+/Ur36dChg7Ru3VoKCgrMbb0+/fTTpUWLFqX7DBgwwCwq9K9//at0n7I/I7JP5GfATjtEaBd0dna2XHLJJbJixQpLzyoY7VAdum/v3r2lbt26UW2lXdrff/+9q88hGcSrHSJ+8YtfmNMKvXr1koULF7p45MnFrXbQnxP5GYrPB3f4Nnzo6rdjx46V888/X7p06WK26VgA/QPZuHHjqH31Ay4yTkCvy37gRe6P3HesffSD8ccff4zr8/KbeLaDBo5nn31WXn31VXPR87B9+vSRjz/+2NKzS/52qI7qtBXi3w46RuHxxx+X+fPny1tvvWXCh45rIIDErx0++OADeemll+Q3v/lN6TY+H9zhuVVtq0sHDa1fv16WL1+e6EMJtHi2Q/v27c0l4rzzzpPNmzfLE088IS+++KLrj+dnvB+Svx2aNm0q48aNK72tgyt37Nghjz32mOkNgbvtoP/+yiuvNCXX+/fv7+rxwac9H7feeqssWrRI/v73v0urVq1Kt2dlZcnBgwdl9+7dUfvraGa9L7JP+VHmkdvH20eXWdYR0LDTDpXp3r27bNq0yeVnEtx2qI6atlXQxLsdKtOjRw/eD3Fohw0bNsjFF19sejzuueeeqPv4fAhg+NABsvrCWrBggbz33nuSl5cXdb9OU6tTp44sXry4dJuel9YpnT179jS39XrdunWya9eu0n3eeecd88Lp1KlT6T5lf0Zkn8jPCDpb7VCZNWvWmNMxcKcdqkP3XbZsmTlfXrattFfq5JNPlqCz1Q6V4f3gfjvomLO+ffvK8OHD5eGHH67wOHw+uMTxkdGjRzsZGRlmGlTZqWY//PBD1FQqnV713nvvmalUPXv2NJfyUzz79+9vpmPp9KhmzZpVOtV2/PjxZjT0tGnTmEqVgHZ44oknnNdff935/PPPnXXr1jm33367mSGjUw3hTjso/f3qDIr/+q//ck477TTz/3qJzG7RWQI61VangOo0xnnz5pn3B1Nt7bbDrFmznDlz5pi/SXp5+OGHzfvh+eeft/6ck7Ud9O+M/h0aOnRo1M/QmTMRfD64w1fhQ7NSZRed2x3x448/mqlROjVQXyBXXXWVefGUtXXrVufSSy8187e1tsQdd9zhHDp0KGofnebZrVs3p27duk7btm2jHiPobLXDI4884rRr186pV6+ek5mZ6fTp08f80YC77XDhhRdW+nN0amfE2rVrnV69ejlpaWlOy5YtnSlTplh9rl5mqx00fHTs2NH8e52S3r1796hpo0HnRjvoNOfKfkabNm2iHovPhxOXov9xqxcFAAAgqcZ8AAAA/yN8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAEJv+P72020TzvF1+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnet_output_pred = nnet(input_train_data)\n",
    "\n",
    "plt.scatter(input_train_data['Year'], nnet_output_pred[0], label=\"Predicted\")\n",
    "plt.scatter(input_train_data['Year'], output_train_data, label=\"Actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One output for average temperature for each year considered\n",
    "output_size = num_years_train\n",
    "\n",
    "nnet = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_train_data.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='relu') # Linear output layer\n",
    "]) # Model with 2 hidden layers with 256 hidden units each (relu activation)\n",
    "nnet.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    loss=temp_loss, # Use binary cross entropy loss for binary classification (all arguements optional)\n",
    "    metrics=['mae']\n",
    ") # Use Binary accuracy as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet.fit(input_train_data, output_train_data, epochs=100)\n",
    "\n",
    "plt.scatter(X[np.equal(Y[:,-1], 0), 0], X[np.equal(Y[:,-1], 0), 1])\n",
    "plt.scatter(X[np.equal(Y[:,-1], 1), 0], X[np.equal(Y[:,-1], 1), 1], marker='+')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
