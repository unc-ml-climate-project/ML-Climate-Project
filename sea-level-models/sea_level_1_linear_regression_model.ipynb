{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from data_cleaning.data_cleaner import DataCleaner\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for Sea Level 1\n",
    "\n",
    "**inputs**: Year, Country, Sea level rise, Average rainfall  \n",
    "**output**: Average temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Avg Temperature (°C)</th>\n",
       "      <th>Sea Level Rise (mm)</th>\n",
       "      <th>Rainfall (mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2047.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>Australia</td>\n",
       "      <td>11.933333</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>2033.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>Canada</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>1383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>China</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1849.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>France</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1819.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000</td>\n",
       "      <td>Germany</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>2641.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>India</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1124.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>23.585714</td>\n",
       "      <td>3.242857</td>\n",
       "      <td>1781.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1974.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year    Country  Avg Temperature (°C)  Sea Level Rise (mm)  Rainfall (mm)\n",
       "0  2000  Argentina             16.900000             4.000000    2047.000000\n",
       "1  2000  Australia             11.933333             2.266667    2033.333333\n",
       "2  2000     Brazil             31.200000             3.700000     803.000000\n",
       "3  2000     Canada             19.300000             2.650000    1383.000000\n",
       "4  2000      China             26.200000             2.200000    1849.000000\n",
       "5  2000     France             16.600000             2.800000    1819.666667\n",
       "6  2000    Germany              9.750000             1.450000    2641.000000\n",
       "7  2000      India             21.250000             3.250000    1124.500000\n",
       "8  2000  Indonesia             23.585714             3.242857    1781.428571\n",
       "9  2000     Mexico             16.900000             1.200000    1974.500000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Go up one directory to the root\n",
    "root = os.path.abspath(os.path.join(current_directory, \"..\"))\n",
    "\n",
    "# Get the path to the data\n",
    "data_path = os.path.join(root, 'clean-data/processed_Sea_level_1_data.csv')\n",
    "\n",
    "dc: DataCleaner = DataCleaner(data_path)\n",
    "dc.preview(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year  Avg Temperature (°C)  Sea Level Rise (mm)  Rainfall (mm)  \\\n",
      "0    2000             16.900000             4.000000    2047.000000   \n",
      "1    2000             11.933333             2.266667    2033.333333   \n",
      "2    2000             31.200000             3.700000     803.000000   \n",
      "3    2000             19.300000             2.650000    1383.000000   \n",
      "4    2000             26.200000             2.200000    1849.000000   \n",
      "..    ...                   ...                  ...            ...   \n",
      "335  2023             20.000000             2.750000    1772.500000   \n",
      "336  2023             30.900000             3.300000     979.000000   \n",
      "337  2023             19.600000             2.700000    1260.500000   \n",
      "338  2023             30.300000             2.750000    1503.000000   \n",
      "339  2023             16.257143             2.671429    1532.000000   \n",
      "\n",
      "     Country_Argentina  Country_Australia  Country_Brazil  Country_Canada  \\\n",
      "0                  1.0                0.0             0.0             0.0   \n",
      "1                  0.0                1.0             0.0             0.0   \n",
      "2                  0.0                0.0             1.0             0.0   \n",
      "3                  0.0                0.0             0.0             1.0   \n",
      "4                  0.0                0.0             0.0             0.0   \n",
      "..                 ...                ...             ...             ...   \n",
      "335                0.0                0.0             0.0             0.0   \n",
      "336                0.0                0.0             0.0             0.0   \n",
      "337                0.0                0.0             0.0             0.0   \n",
      "338                0.0                0.0             0.0             0.0   \n",
      "339                0.0                0.0             0.0             0.0   \n",
      "\n",
      "     Country_China  Country_France  Country_Germany  Country_India  \\\n",
      "0              0.0             0.0              0.0            0.0   \n",
      "1              0.0             0.0              0.0            0.0   \n",
      "2              0.0             0.0              0.0            0.0   \n",
      "3              0.0             0.0              0.0            0.0   \n",
      "4              1.0             0.0              0.0            0.0   \n",
      "..             ...             ...              ...            ...   \n",
      "335            0.0             0.0              0.0            0.0   \n",
      "336            0.0             0.0              0.0            0.0   \n",
      "337            0.0             0.0              0.0            0.0   \n",
      "338            0.0             0.0              0.0            0.0   \n",
      "339            0.0             0.0              0.0            0.0   \n",
      "\n",
      "     Country_Indonesia  Country_Japan  Country_Mexico  Country_Russia  \\\n",
      "0                  0.0            0.0             0.0             0.0   \n",
      "1                  0.0            0.0             0.0             0.0   \n",
      "2                  0.0            0.0             0.0             0.0   \n",
      "3                  0.0            0.0             0.0             0.0   \n",
      "4                  0.0            0.0             0.0             0.0   \n",
      "..                 ...            ...             ...             ...   \n",
      "335                0.0            0.0             1.0             0.0   \n",
      "336                0.0            0.0             0.0             1.0   \n",
      "337                0.0            0.0             0.0             0.0   \n",
      "338                0.0            0.0             0.0             0.0   \n",
      "339                0.0            0.0             0.0             0.0   \n",
      "\n",
      "     Country_South Africa  Country_UK  Country_USA  \n",
      "0                     0.0         0.0          0.0  \n",
      "1                     0.0         0.0          0.0  \n",
      "2                     0.0         0.0          0.0  \n",
      "3                     0.0         0.0          0.0  \n",
      "4                     0.0         0.0          0.0  \n",
      "..                    ...         ...          ...  \n",
      "335                   0.0         0.0          0.0  \n",
      "336                   0.0         0.0          0.0  \n",
      "337                   1.0         0.0          0.0  \n",
      "338                   0.0         1.0          0.0  \n",
      "339                   0.0         0.0          1.0  \n",
      "\n",
      "[340 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "df_copy = dc.df.copy()\n",
    "\n",
    "# One Hot Encode the country column\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "country_encoded = encoder.fit_transform(dc.df[['Country']])\n",
    "country_columns = encoder.get_feature_names_out(['Country'])\n",
    "\n",
    "df_encoded = pd.concat([dc.df.drop(columns=['Country']), pd.DataFrame(country_encoded, columns=country_columns)], axis=1)\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X = df_encoded[\"Year\"].values.reshape(-1, 1) # 2D column vector\n",
    "X = np.hstack((X, df_encoded.drop(columns=[\"Avg Temperature (°C)\", \"Year\"]).values)) # remove target column and year column (already included) stack year and other features horizontally, shape: (n_samples, total_features)\n",
    "Y = df_encoded[\"Avg Temperature (°C)\"].values.reshape(-1, 1) # 2D column vector\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_dim=X.shape[1], activation='linear')  # Linear Regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1098451.8750 - val_loss: 822835.5000\n",
      "Epoch 2/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 790235.3750 - val_loss: 582508.7500\n",
      "Epoch 3/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 555668.6875 - val_loss: 397430.0312\n",
      "Epoch 4/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 385741.7188 - val_loss: 263271.0938\n",
      "Epoch 5/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 246181.3281 - val_loss: 168656.3438\n",
      "Epoch 6/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 154412.1094 - val_loss: 104673.4375\n",
      "Epoch 7/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92600.6328 - val_loss: 63153.3086\n",
      "Epoch 8/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64287.2344 - val_loss: 37282.6875\n",
      "Epoch 9/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34333.9570 - val_loss: 22275.6914\n",
      "Epoch 10/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19142.5625 - val_loss: 13547.7686\n",
      "Epoch 11/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11512.3867 - val_loss: 8730.1357\n",
      "Epoch 12/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9381.8398 - val_loss: 6392.4263\n",
      "Epoch 13/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6722.5503 - val_loss: 5243.8828\n",
      "Epoch 14/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5857.9805 - val_loss: 4634.4688\n",
      "Epoch 15/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5058.5806 - val_loss: 4352.5649\n",
      "Epoch 16/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4383.7505 - val_loss: 4200.7241\n",
      "Epoch 17/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4373.4712 - val_loss: 4090.6704\n",
      "Epoch 18/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4740.0225 - val_loss: 4005.9097\n",
      "Epoch 19/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4275.8687 - val_loss: 3922.6687\n",
      "Epoch 20/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4635.8462 - val_loss: 3839.5659\n",
      "Epoch 21/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4546.0884 - val_loss: 3752.7893\n",
      "Epoch 22/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3875.2166 - val_loss: 3670.6265\n",
      "Epoch 23/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4382.7983 - val_loss: 3581.4429\n",
      "Epoch 24/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4128.4043 - val_loss: 3490.9932\n",
      "Epoch 25/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4402.0083 - val_loss: 3399.6323\n",
      "Epoch 26/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3853.7856 - val_loss: 3311.8198\n",
      "Epoch 27/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3913.7878 - val_loss: 3228.6538\n",
      "Epoch 28/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3952.9651 - val_loss: 3137.9780\n",
      "Epoch 29/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3109.5122 - val_loss: 3042.9119\n",
      "Epoch 30/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3372.2673 - val_loss: 2953.1462\n",
      "Epoch 31/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3282.0735 - val_loss: 2860.3728\n",
      "Epoch 32/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3123.4304 - val_loss: 2769.7988\n",
      "Epoch 33/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3088.4412 - val_loss: 2676.1338\n",
      "Epoch 34/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2813.6108 - val_loss: 2591.0630\n",
      "Epoch 35/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3092.2874 - val_loss: 2503.4517\n",
      "Epoch 36/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2824.7297 - val_loss: 2407.7998\n",
      "Epoch 37/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2807.4529 - val_loss: 2322.7292\n",
      "Epoch 38/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2453.2695 - val_loss: 2237.9092\n",
      "Epoch 39/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2283.8149 - val_loss: 2150.1733\n",
      "Epoch 40/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2530.6272 - val_loss: 2062.5044\n",
      "Epoch 41/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2506.9480 - val_loss: 1976.0287\n",
      "Epoch 42/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2073.3071 - val_loss: 1900.3718\n",
      "Epoch 43/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2029.5592 - val_loss: 1814.9923\n",
      "Epoch 44/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1985.2303 - val_loss: 1739.1355\n",
      "Epoch 45/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2097.6682 - val_loss: 1660.6340\n",
      "Epoch 46/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1990.5812 - val_loss: 1584.6982\n",
      "Epoch 47/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1959.6123 - val_loss: 1511.2596\n",
      "Epoch 48/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1801.2451 - val_loss: 1434.9865\n",
      "Epoch 49/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1682.7623 - val_loss: 1365.8849\n",
      "Epoch 50/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1557.5579 - val_loss: 1301.3281\n",
      "Epoch 51/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1419.2964 - val_loss: 1233.7395\n",
      "Epoch 52/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1248.1459 - val_loss: 1170.7115\n",
      "Epoch 53/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1339.5809 - val_loss: 1109.9906\n",
      "Epoch 54/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1207.4146 - val_loss: 1043.6576\n",
      "Epoch 55/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1021.6608 - val_loss: 986.5757\n",
      "Epoch 56/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1084.0181 - val_loss: 930.7753\n",
      "Epoch 57/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1041.8207 - val_loss: 875.8335\n",
      "Epoch 58/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 901.3413 - val_loss: 825.8325\n",
      "Epoch 59/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 880.0364 - val_loss: 776.6933\n",
      "Epoch 60/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 908.7449 - val_loss: 727.7064\n",
      "Epoch 61/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 859.9360 - val_loss: 682.7817\n",
      "Epoch 62/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 667.8511 - val_loss: 639.2786\n",
      "Epoch 63/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 767.3230 - val_loss: 598.1775\n",
      "Epoch 64/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 689.8036 - val_loss: 558.4844\n",
      "Epoch 65/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 668.4985 - val_loss: 520.3298\n",
      "Epoch 66/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 551.9398 - val_loss: 485.0458\n",
      "Epoch 67/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 526.0734 - val_loss: 450.6026\n",
      "Epoch 68/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 445.7387 - val_loss: 421.1664\n",
      "Epoch 69/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 460.5532 - val_loss: 388.6324\n",
      "Epoch 70/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 478.9348 - val_loss: 358.2129\n",
      "Epoch 71/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 405.8983 - val_loss: 333.5916\n",
      "Epoch 72/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 366.4294 - val_loss: 308.1890\n",
      "Epoch 73/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 387.5756 - val_loss: 285.7634\n",
      "Epoch 74/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 308.9121 - val_loss: 263.1939\n",
      "Epoch 75/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 295.7990 - val_loss: 243.1086\n",
      "Epoch 76/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 259.9470 - val_loss: 223.2851\n",
      "Epoch 77/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 229.7733 - val_loss: 206.2759\n",
      "Epoch 78/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 230.1575 - val_loss: 188.9761\n",
      "Epoch 79/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 240.4118 - val_loss: 173.7328\n",
      "Epoch 80/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 221.9081 - val_loss: 159.1996\n",
      "Epoch 81/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 158.8537 - val_loss: 147.5916\n",
      "Epoch 82/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.1007 - val_loss: 137.3479\n",
      "Epoch 83/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.0571 - val_loss: 125.7962\n",
      "Epoch 84/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.7724 - val_loss: 114.9166\n",
      "Epoch 85/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 120.1487 - val_loss: 105.9819\n",
      "Epoch 86/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 119.0988 - val_loss: 97.6681\n",
      "Epoch 87/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94.7242 - val_loss: 90.6133\n",
      "Epoch 88/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89.1774 - val_loss: 85.2879\n",
      "Epoch 89/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 93.0788 - val_loss: 78.8533\n",
      "Epoch 90/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.6339 - val_loss: 72.8649\n",
      "Epoch 91/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.5191 - val_loss: 69.4273\n",
      "Epoch 92/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60.7020 - val_loss: 64.2584\n",
      "Epoch 93/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64.5377 - val_loss: 60.2847\n",
      "Epoch 94/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.6207 - val_loss: 57.0883\n",
      "Epoch 95/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61.8827 - val_loss: 54.8076\n",
      "Epoch 96/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47.8807 - val_loss: 51.2470\n",
      "Epoch 97/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46.2229 - val_loss: 49.1544\n",
      "Epoch 98/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46.6867 - val_loss: 46.8746\n",
      "Epoch 99/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44.5309 - val_loss: 45.2248\n",
      "Epoch 100/100\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45.2825 - val_loss: 43.5916\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=5, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test data\n",
    "# test_loss = model.evaluate(X_test, Y_test)\n",
    "# print(f\"Test Loss (MSE): {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Y Predictions:\n",
      "[[18.992044]\n",
      " [19.354977]\n",
      " [21.439741]\n",
      " [18.653927]\n",
      " [21.714447]\n",
      " [20.84709 ]\n",
      " [15.448186]\n",
      " [18.062258]\n",
      " [22.541473]\n",
      " [22.44096 ]]\n",
      "Y Test:\n",
      "[[33.3       ]\n",
      " [19.2       ]\n",
      " [12.05      ]\n",
      " [12.06666667]\n",
      " [21.9       ]\n",
      " [17.275     ]\n",
      " [15.4       ]\n",
      " [26.8       ]\n",
      " [19.6       ]\n",
      " [26.65      ]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Y Predictions:\\n{Y_pred[:10]}\")\n",
    "print(f\"Y Test:\\n{Y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: -0.0927\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Flatten arrays for plotting\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m Y_test_flat \u001b[38;5;241m=\u001b[39m \u001b[43mY_test\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      3\u001b[0m Y_pred_flat \u001b[38;5;241m=\u001b[39m Y_pred\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot: Actual vs Predicted\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Flatten arrays for plotting\n",
    "Y_test_flat = Y_test.flatten()\n",
    "Y_pred_flat = Y_pred.flatten()\n",
    "\n",
    "# Plot: Actual vs Predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Y_test_flat, Y_pred_flat, color='blue', alpha=0.6, label='Predicted Values')\n",
    "\n",
    "# Line of equality (ideal prediction line)\n",
    "plt.plot([min(Y_test_flat), max(Y_test_flat)], [min(Y_test_flat), max(Y_test_flat)], \n",
    "         color='red', linestyle='--', label='Equality Line')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Actual Avg Temperature (°C)')\n",
    "plt.ylabel('Predicted Avg Temperature (°C)')\n",
    "plt.title('Actual vs Predicted Avg Temperature')\n",
    "\n",
    "# Grid and legend\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(\"lin_reg_temp_trend.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
